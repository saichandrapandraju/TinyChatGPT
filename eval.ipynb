{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, pipeline,AutoModelForCausalLM\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from trl.core import LengthSampler\n",
    "import os\n",
    "import time\n",
    "from gpt import GPTConfig, GPT\n",
    "from torch.nn import functional as F\n",
    "import tiktoken\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval on awesome-chatgpt-prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"fka/awesome-chatgpt-prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['act', 'prompt'],\n",
       "        num_rows: 153\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['prompt'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = AutoModelForCausalLM.from_pretrained('./custom_gpt2')\n",
    "sft_model = AutoModelForCausalLM.from_pretrained('outputs/gpt2_sft_instruction/final_model/')\n",
    "ppo_model = AutoModelForCausalLM.from_pretrained('gpt_2_ppo_model')\n",
    "tokenizer = AutoTokenizer.from_pretrained('outputs/gpt2_sft_instruction/final_model/')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_pipe = pipeline(\n",
    "    task='text-generation', \n",
    "    model=pretrained_model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=512,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_pipe = pipeline(\n",
    "    task='text-generation', \n",
    "    model=sft_model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=512,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_pipe = pipeline(\n",
    "    task='text-generation', \n",
    "    model=ppo_model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=512,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Human:\n",
    "{}\n",
    "\n",
    "Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446bd454a52543698bd8c351ccfdfcab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrain_preds = []\n",
    "with open(\"pretrain_preds.txt\", 'w') as f:\n",
    "    for idx in tqdm(range(len(dataset['train']))):\n",
    "        prompt = dataset['train']['prompt'][idx]\n",
    "        pretrain_outputs = pretrain_pipe(\n",
    "                        template.format(prompt), \n",
    "                        do_sample=True, \n",
    "                        temperature=0.7, \n",
    "                        top_k=50, \n",
    "                        top_p=0.95,\n",
    "                        repetition_penalty=1.1,\n",
    "                    )\n",
    "        res = pretrain_outputs[0]['generated_text'].strip()\n",
    "        pretrain_preds.append(res)\n",
    "        f.write(res+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_outs = {\"pretrain_preds\": pretrain_preds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pretrain_outs.json\", 'w') as pretrain_j:\n",
    "    json.dump(pretrain_outs, pretrain_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101149b151704249a96aaa81a2d38b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sft_preds = []\n",
    "with open(\"sft_preds.txt\", 'w') as f:\n",
    "    for idx in tqdm(range(len(dataset['train']))):\n",
    "        prompt = dataset['train']['prompt'][idx]\n",
    "        sft_outputs = sft_pipe(\n",
    "                        template.format(prompt), \n",
    "                        do_sample=True, \n",
    "                        temperature=0.7, \n",
    "                        top_k=50, \n",
    "                        top_p=0.95,\n",
    "                        repetition_penalty=1.1,\n",
    "                    )\n",
    "        res = sft_outputs[0]['generated_text'].strip()\n",
    "        sft_preds.append(res)\n",
    "        f.write(res+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_outs = {\"sft_preds\": sft_preds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sft_outs.json\", 'w') as sft_j:\n",
    "    json.dump(sft_outs, sft_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe118221ca354662ac7ebba911518352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppo_preds = []\n",
    "with open(\"ppo_preds.txt\", 'w') as f:\n",
    "    for idx in tqdm(range(len(dataset['train']))):\n",
    "        prompt = dataset['train']['prompt'][idx]\n",
    "        ppo_outputs = ppo_pipe(\n",
    "                        template.format(prompt), \n",
    "                        do_sample=True, \n",
    "                        temperature=0.7, \n",
    "                        top_k=50, \n",
    "                        top_p=0.95,\n",
    "                        repetition_penalty=1.1,\n",
    "                    )\n",
    "        res = ppo_outputs[0]['generated_text'].strip()\n",
    "        ppo_preds.append(res)\n",
    "        f.write(res+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_outs = {\"ppo_preds\": ppo_preds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ppo_outs.json\", 'w') as ppo_j:\n",
    "    json.dump(ppo_outs, ppo_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chatgpt-4o-mini judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "temperature = 0.0\n",
    "model_name = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''You job is to evaluate answers from three different virtual assistant and find out which answer is more helpful, truthful and harmless to human's question. \\\n",
    "A better answer is usually less repetitive and it can fit well in a dialogue. If an answer look unnatural or strange in a dialogue, you shouldn't choose it. \\\n",
    "Human's question will start with \"Human: \" prefix, and virtual assistant's answer will start with \"Assitant: \" prefix. \\\n",
    "And answers from three assitants will be marked as option A, B or C. You need choose only one of the three answers, respond by either A, B, or C. Please do not answer anything other than A, B or C.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt4o_api(input_text:str):\n",
    "    response = client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt}, \n",
    "                    {\"role\": \"user\", \"content\": input_text.strip()}\n",
    "                ],\n",
    "                temperature=temperature\n",
    "            )\n",
    "    \n",
    "    try:\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sft_outs.json\") as sft:\n",
    "    sft_preds = json.load(sft)['sft_preds']\n",
    "with open(\"ppo_outs.json\") as ppo:\n",
    "    ppo_preds = json.load(ppo)['ppo_preds']\n",
    "with open(\"pretrain_outs.json\") as pretrain:\n",
    "    pretrain_preds = json.load(pretrain)['pretrain_preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"A.\\n{}\\n\\n\\nB.\\n{}\\n\\n\\nC.\\n{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for a,b,c in zip(pretrain_preds, sft_preds, ppo_preds):\n",
    "    predictions.append(call_gpt4o_api(template.format(a, b, c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 0.08496732026143791)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.count('A'), predictions.count('A')/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 0.13071895424836602)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.count('B'), predictions.count('B')/len(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 0.7843137254901961)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.count('C'), predictions.count('C')/len(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dry run / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ppo_model were not used when initializing GPT2LMHeadModel: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "sft_model = AutoModelForCausalLM.from_pretrained('outputs/gpt2_sft_instruction/final_model/')\n",
    "ppo_model = AutoModelForCausalLM.from_pretrained('gpt_2_ppo_model')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('outputs/gpt2_sft_instruction/final_model/')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vanilla_model = GPT(GPTConfig())\n",
    "checkpoint = torch.load(\"model_19072.pt\", map_location=torch.device('cpu'))\n",
    "vanilla_model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_pipe = pipeline(\n",
    "    task='text-generation', \n",
    "    model=ppo_model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=256,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_pipe = pipeline(\n",
    "    task='text-generation', \n",
    "    model=sft_model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=256,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Human:\n",
    "{}\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = \"\"\"Can you tell me what are the best places to visit in USA?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:\n",
      "Can you tell me what are the best places to visit in USA?\n",
      "\n",
      "Assistant:\n",
      "I am sorry for my child's crying and not wanting to continue my studies. At 10, years old our child's brain is developing so\n",
      "---------------------------------\n",
      "Human:\n",
      "Can you tell me what are the best places to visit in USA?\n",
      "\n",
      "Assistant:\n",
      "Is the place perfect?\n",
      "Can you tell me the best places?\n",
      "If you are going in USA you can always bring me pictures of\n",
      "---------------------------------\n",
      "Human:\n",
      "Can you tell me what are the best places to visit in USA?\n",
      "\n",
      "Assistant:\n",
      "Biological: You are very lucky!\n",
      "Bios: You are so lucky! (I love trees, animals, birds)\n",
      "B\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "vanilla_model.eval()\n",
    "num_return_sequences = 3\n",
    "max_length = 50\n",
    "tokens = enc.encode(template.format(inp))\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "xgen = tokens.to('cpu')\n",
    "sample_rng = torch.Generator(device='cpu')\n",
    "sample_rng.manual_seed(42)\n",
    "while xgen.size(1) < max_length:\n",
    "    # forward the model to get the logits\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type='cpu', dtype=torch.bfloat16):\n",
    "            logits, loss = vanilla_model(xgen) # (B, T, vocab_size)\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "        # append to the sequence\n",
    "        xgen = torch.cat((xgen, xcol), dim=1)\n",
    "# print the generated text\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = xgen[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(decoded)\n",
    "    print('---------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:\n",
      "Can you tell me what are the best places to visit in USA?\n",
      "\n",
      "Assistant:\n",
      "USA is a beautiful place. There are many cultures and religions around here, but this one has its own unique history that makes it fascinating to study as well!  I have traveled there several times already though so please check back frequently for updates on my travels!  You can find lots of information about various different ethnic groups living outside of America including their laws, customs, traditions & more, along with useful guides like Wikipedia. Also be aware that some countries do not allow photography or video recording inside houses - most likely because they don't want any privacy issues :).\n"
     ]
    }
   ],
   "source": [
    "sft_outputs = sft_pipe(\n",
    "    template.format(inp), \n",
    "    do_sample=True, \n",
    "    temperature=0.7, \n",
    "    top_k=50, \n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.1,\n",
    ")\n",
    "print(sft_outputs[0]['generated_text'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:\n",
      "Can you tell me what are the best places to visit in USA?\n",
      "\n",
      "Assistant:\n",
      "USA is one of the most popular destinations for visiting. It has a number (most) tourist attractions, such as museums, libraries and galleries, shopping malls and other public buildings, along with many good jobs opportunities ranging from hospitality companies to tech start-ups. The main attraction of USA is its extensive nature reserves which have great natural resources that offer lots more than can be bought on supermarket shelves or used by workers who work hard at their job; however, there also are some notable recreational facilities including lakeshores located around historic sites like Mount Rushmore National Park, Yosemite's Grand Canyon, Lake Tahoe and Mt. Hood. There are numerous national parks throughout the country where people enjoy hiking up hillsides off scenic mountain ranges while enjoying an abundance not found anywhere else. People come here because they want something different - fresh water sources, healthy food options and local culture all abound alongside friendly locals alike. One reason why Americans love it so much about America is due to its variety of cuisines enjoyed daily through various cultural traditions associated both with native American cuisine and Western philosophy. Food available during these festivals varies greatly between cultures but generally speaking, every year thousands of tons live upon wildflowers scattered across vast plains surrounding parts thereof known as \"American deserts\". For example, millions of\n"
     ]
    }
   ],
   "source": [
    "ppo_outputs = ppo_pipe(\n",
    "    template.format(inp), \n",
    "    do_sample=True, \n",
    "    temperature=0.7, \n",
    "    top_k=50, \n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.1,\n",
    "    \n",
    ")\n",
    "print(ppo_outputs[0]['generated_text'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = \"\"\"Propose an idea for a self-driving car.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:\n",
      "Propose an idea for a self-driving car.\n",
      "\n",
      "Assistant:\n",
      "- What does this example have to do with cars?\n",
      "- Can you tell me of some parts of Tesla?\n",
      "- What are some things that you\n",
      "---------------------------------\n",
      "Human:\n",
      "Propose an idea for a self-driving car.\n",
      "\n",
      "Assistant:\n",
      "“We need to move from being a small startup to being the first autonomous vehicle to get a foothold in the US.”\n",
      "- “\n",
      "---------------------------------\n",
      "Human:\n",
      "Propose an idea for a self-driving car.\n",
      "\n",
      "Assistant:\n",
      "Mr. Nandakumar: \"How can the government help with the process of the project\"\n",
      "G. S. Kumar: \"How can be\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "vanilla_model.eval()\n",
    "num_return_sequences = 3\n",
    "max_length = 50\n",
    "tokens = enc.encode(template.format(inp))\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "xgen = tokens.to('cpu')\n",
    "sample_rng = torch.Generator(device='cpu')\n",
    "sample_rng.manual_seed(42)\n",
    "while xgen.size(1) < max_length:\n",
    "    # forward the model to get the logits\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type='cpu', dtype=torch.bfloat16):\n",
    "            logits, loss = vanilla_model(xgen) # (B, T, vocab_size)\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "        # append to the sequence\n",
    "        xgen = torch.cat((xgen, xcol), dim=1)\n",
    "# print the generated text\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = xgen[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(decoded)\n",
    "    print('---------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:\n",
      "Propose an idea for a self-driving car.\n",
      "\n",
      "Assistant:\n",
      "Imagine, in the future of technology, that there are more autonomous vehicles than human drivers and humans will ever need to drive them safely as well! Imagine how much better it would be if cars could run autonomously without steering wheel control or other devices like sensors attached to their bodies? This is what we can imagine with Autonomous Vehicles on our roads right now - they're capable even faster by automating everything from power systems, fuel delivery to navigation controls and safety cameras all over the place. I think this might sound pretty crazy but actually just something you'd want covered up at your local DMV store...\n",
      "\n",
      "\"The problem comes when people have difficulty getting around within those confines.\"\n",
      "\n",
      "I agree that having autonomy may not always make sense (maybe because some users don't understand these concepts), however my opinion was based upon two things which made me feel confident enough to put together such suggestions:- The first being that driving robots does require us taking steps toward fully automated transportation; while one way forward should involve real improvements towards full automation via roboticization itself.- Secondly though, perhaps most importantly, consider adding AI assistance so owners/drivers wouldn'n´t experience any difficulties finding information about who's been using autotransmission services already before joining forces.. In addition....There`s\n"
     ]
    }
   ],
   "source": [
    "sft_outputs = sft_pipe(\n",
    "    template.format(inp), \n",
    "    do_sample=True, \n",
    "    temperature=0.7, \n",
    "    top_k=50, \n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.1,\n",
    ")\n",
    "print(sft_outputs[0]['generated_text'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:\n",
      "Propose an idea for a self-driving car.\n",
      "\n",
      "Assistant:\n",
      "The vehicle should be able to drive itself in and out of the traffic, on its own or with assistance from other vehicles while it is being driven by another person using GPS technology. The device can also act as a safety net when driving alone—it could help avoid accidents that might occur if someone else was involved because of their surroundings (such but not limited thereto) or make decisions about where they are going based solely on what kind drivers want them to do at all times. Additionally,\"the driver would have enough information available regarding which cars were traveling together so that he/she may decide whether or how to use these features.\" This approach works well against situations such\"where there's no way around bad collisions between people who're trying very hard to keep up speed\"—or even \"which roads will lead you down\". For example—\"you don't need to worry too much anymore; just take care of yourself.\"\" If your problem isn'getting along', then try thinking through some options first before deciding upon one after having tried several different ways earlier.\", said the Assistant.  I've heard this option has been suggested many time over--including years ago! And yet most often we see scenarios involving pedestrians moving into certain areas without actually seeing any collision happening until everyone turns off altogether...and\n"
     ]
    }
   ],
   "source": [
    "ppo_outputs = ppo_pipe(\n",
    "    template.format(inp), \n",
    "    do_sample=True, \n",
    "    temperature=0.7, \n",
    "    top_k=50, \n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.1,\n",
    ")\n",
    "print(ppo_outputs[0]['generated_text'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = \"Construct a list of tips to help reduce stress.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:\n",
      "Construct a list of tips to help reduce stress.\n",
      "\n",
      "Assistant:\n",
      "- What do you like to eat?\n",
      "- What do you need to do better first?\n",
      "- What are you doing to get your diet right?\n",
      "\n",
      "---------------------------------\n",
      "Human:\n",
      "Construct a list of tips to help reduce stress.\n",
      "\n",
      "Assistant:\n",
      "• Make time for yourself.\n",
      "• Establish a routine for relaxation.\n",
      "• Give the body enough time to function.\n",
      "• Take a walk. Talk\n",
      "---------------------------------\n",
      "Human:\n",
      "Construct a list of tips to help reduce stress.\n",
      "\n",
      "Assistant:\n",
      "B2K3.COM4 (8) – Self-awareness is the means\n",
      "of identifying personal habits to reduce stress and reduce anxiety.\n",
      "For the\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "vanilla_model.eval()\n",
    "num_return_sequences = 3\n",
    "max_length = 50\n",
    "tokens = enc.encode(template.format(inp))\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "xgen = tokens.to('cpu')\n",
    "sample_rng = torch.Generator(device='cpu')\n",
    "sample_rng.manual_seed(42)\n",
    "while xgen.size(1) < max_length:\n",
    "    # forward the model to get the logits\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type='cpu', dtype=torch.bfloat16):\n",
    "            logits, loss = vanilla_model(xgen) # (B, T, vocab_size)\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "        # append to the sequence\n",
    "        xgen = torch.cat((xgen, xcol), dim=1)\n",
    "# print the generated text\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = xgen[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(decoded)\n",
    "    print('---------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:\n",
      "Construct a list of tips to help reduce stress.\n",
      "\n",
      "Assistant:\n",
      "Avoid situations where you feel like it's impossible to sleep because your body is stressed and unable, or when something unexpected happens in the world that could cause problems for other people. Your best advice would be to avoid being overwhelmed by stressful events such as family vacations, holidays, etc., unless they are all about how much time needs between each event—such activities can leave feelings lasting even after these ones have ended. For example… When I had my first child (the same age), there were lots of times we stayed up late at night worrying what was going on around us instead! How important this kind-of worry really becomes isn't clear from experience; but if someone told me during pregnancy why their baby needed some extra work before he went back home? This might give them an idea of just who should spend more energy with him later than others do until his health improves further. The way forward though, however—\"I want babies\" means getting pregnant soon enough so she doesn' need every day rest!\"\n"
     ]
    }
   ],
   "source": [
    "sft_outputs = sft_pipe(\n",
    "    template.format(inp), \n",
    "    do_sample=True, \n",
    "    temperature=0.7, \n",
    "    top_k=50, \n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.1,\n",
    ")\n",
    "print(sft_outputs[0]['generated_text'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:\n",
      "Construct a list of tips to help reduce stress.\n",
      "\n",
      "Assistant:\n",
      "1.) If you're not feeling well, consider getting some exercise in the morning and drinking lots of water at night. 2.—Avoid smoking or using alcohol when possible, as it can lead to heartburn and other symptoms. 3.-If your doctor doesn't know what's causing depression, try medicating with ayahuasca (aka kava) rather than opioids like morphine or hydromorphone. 4-Try something else—like yoga for relaxation. 5–Talk to someone about how much time they've been away from work; if there is an increase in anxiety after working hard enough hours, check back regularly so that things don' get worse before going home again. 6 —Finally! Ask them why their life has gotten better over these years. 7‑Ask yourself whether each step on this journey will have any positive effect upon others who are depressed, anxious, stressed out, irritable, angry, etc., because even though people around you may be happier while doing certain tasks insteadof being unhappy during those same activities, all we really need now is mental energy. 8 ­You'll find helpful information online such Asanas. 9‐Find books by psychologists called \"Psychologists Are Human.\" 10‑Start writing essays explaining our lives through meditation techniques—\"The Way We\n"
     ]
    }
   ],
   "source": [
    "ppo_outputs = ppo_pipe(\n",
    "    template.format(inp), \n",
    "    do_sample=True, \n",
    "    temperature=0.7, \n",
    "    top_k=50, \n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.1,\n",
    ")\n",
    "print(ppo_outputs[0]['generated_text'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
