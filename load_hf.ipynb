{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "from gpt import GPTConfig, GPT\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = torch.load(\"model_19072.pt\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(GPTConfig(vocab_size=50304))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"model_19072.pt\", map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "device_type = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Can you tell me what are the best places to visit in USA?\n",
      "\n",
      "Assistant: I can visit you there.\n",
      "Can you show me the Best Places to Visit in USA?\n",
      "Assistant: Yes, I can show you the best\n",
      "---------------------------------\n",
      "Human: Can you tell me what are the best places to visit in USA?\n",
      "\n",
      "Assistant: If a friend was to sit on my back, would you like a place for him? Would you like the place where he sat?\n",
      "Student:\n",
      "---------------------------------\n",
      "Human: Can you tell me what are the best places to visit in USA?\n",
      "\n",
      "Assistant: How many years from the time of your father’s death can you go to a place now without making a trip?\n",
      "I: I always\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "num_return_sequences = 3\n",
    "max_length = 50\n",
    "tokens = enc.encode(\"Human: Can you tell me what are the best places to visit in USA?\\n\\nAssistant:\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "xgen = tokens.to(device)\n",
    "sample_rng = torch.Generator(device=device)\n",
    "sample_rng.manual_seed(42)\n",
    "while xgen.size(1) < max_length:\n",
    "    # forward the model to get the logits\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "            logits, loss = model(xgen) # (B, T, vocab_size)\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "        # append to the sequence\n",
    "        xgen = torch.cat((xgen, xcol), dim=1)\n",
    "# print the generated text\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = xgen[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(decoded)\n",
    "    print('---------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Construct a list of tips to help reduce stress.\n",
      "\n",
      "Assistant: How to handle yourself.\n",
      "Hate: Help others.\n",
      "The following are ideas the teacher or a peer may use to help them identify their own emotional reactions to stress\n",
      "---------------------------------\n",
      "Human: Construct a list of tips to help reduce stress.\n",
      "\n",
      "Assistant: “How much do you need to take into your house to be safe?” (I would suggest you start and keep track of the amount of chores)\n",
      "\n",
      "---------------------------------\n",
      "Human: Construct a list of tips to help reduce stress.\n",
      "\n",
      "Assistant: Dr. Sankaran (firstname.lastname@example.org)\n",
      "Ages: 16 - 25\n",
      "Adolescence: 12 -19 years\n",
      "Health\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "num_return_sequences = 3\n",
    "max_length = 50\n",
    "tokens = enc.encode(\"Human: Construct a list of tips to help reduce stress.\\n\\nAssistant:\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "xgen = tokens.to(device)\n",
    "sample_rng = torch.Generator(device=device)\n",
    "sample_rng.manual_seed(42)\n",
    "while xgen.size(1) < max_length:\n",
    "    # forward the model to get the logits\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "            logits, loss = model(xgen) # (B, T, vocab_size)\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "        # append to the sequence\n",
    "        xgen = torch.cat((xgen, xcol), dim=1)\n",
    "# print the generated text\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = xgen[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(decoded)\n",
    "    print('---------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTConfig(block_size=1024, vocab_size=50257, n_layer=12, n_head=12, n_embd=768)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPTConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = GPT2LMHeadModel(config=GPT2Config(vocab_size=50304))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transpose_weights(checkpoint):\n",
    "    for key in checkpoint.keys():\n",
    "        if \"weight\" in key and checkpoint[key].dim() == 2 :\n",
    "            checkpoint[key] = checkpoint[key].t()\n",
    "    return checkpoint\n",
    "\n",
    "# def transpose_specific_weights(checkpoint):\n",
    "#     for key in checkpoint.keys():\n",
    "#         if key in [\"transformer.wte.weight\", \"transformer.wpe.weight\", \"lm_head.weight\"]:\n",
    "#             checkpoint[key] = checkpoint[key].t()\n",
    "#     return checkpoint\n",
    "\n",
    "# Transpose the weights\n",
    "# transposed_wghts = transpose_weights(transpose_weights(checkpoint['model']))\n",
    "transposed_wghts = transpose_weights(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.load_state_dict(transposed_wghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.save_pretrained(\"./custom_gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12982,    25,  1867,   284,   466,   287,  8031,    30,   198, 48902,\n",
       "            25,  1867,   314,   892,   481,   307,   262,  1388,  3580,    13,\n",
       "           314,   836,   470,  4601,   284,   307,   994,   287,  8031,    11,\n",
       "           655,   994],\n",
       "        [12982,    25,  1867,   284,   466,   287,  8031,    30,   198, 48902,\n",
       "            25,   632,   447,   247,    82,   257,  4950,    11,  4950,  1295,\n",
       "            11,   314,   447,   247,   303,  1239,  1107,  7891,  6609,   881,\n",
       "           286,   262],\n",
       "        [12982,    25,  1867,   284,   466,   287,  8031,    30,   198, 48902,\n",
       "            25,  8200,  5537,  4139,    11,  8200,  5537,  4139,    11,  8200,\n",
       "          5537,  4139,    11,  8200,  5537,  4139,   318,   257,  1049, 14971,\n",
       "           290,   257],\n",
       "        [12982,    25,  1867,   284,   466,   287,  8031,    30,   198, 48902,\n",
       "            25,  1867,   466,   345,   466,   618,   345,  3067,    30,   198,\n",
       "           464,  1708,   318,  5220,  1321,   618,  3067,   318,  6693,    25,\n",
       "           198,    12]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = enc.encode(\"Hello, I'm a scientist and work on\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "xgen = tokens.to(device)\n",
    "max_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "while xgen.size(1) < max_length:\n",
    "    logits = hf_model.forward(xgen)[0][:, -1, :] # (B, vocab_size)\n",
    "    # get the probabilities\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    # do top-k sampling of 50 (huggingface pipeline default)\n",
    "    # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "    topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "    # select a token from the top-k probabilities\n",
    "    # note: multinomial does not demand the input to sum to 1\n",
    "    ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
    "    # gather the corresponding indices\n",
    "    xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "    # append to the sequence\n",
    "    xgen = torch.cat((xgen, xcol), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm a scientist and work on the hand. For\n",
      "’s at the level.\n",
      "… go? And the same. in everything.\n",
      "the world when the. I happen what.\n",
      "I. for in\n",
      "what.\n",
      "Hello, I'm a scientist and work on for an ever so many who, it all those to be used to work, the same things on earth as it is to have that no is how is.\n",
      "A scientist,\n",
      "“ (1\n",
      "Hello, I'm a scientist and work on a man with a lot more than is.\n",
      "This won’t the sea to the name and that can win!\n",
      "Follow it with, can or is only the follow through there�t an\n",
      "Hello, I'm a scientist and work on the to and from . . It really it are and have your thing and , and then the one, and one by so. One - say that was a way here or on the world- that were\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    tokens = xgen[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm a language model, and the kids are all talking to each other on the topic you mentioned: it's just a language. We get the idea that our children should be the same as any other children and the same as any other\n"
     ]
    }
   ],
   "source": [
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs = hf_model.generate(xgen, max_new_tokens=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
